{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-16T12:55:12.147978Z",
     "start_time": "2024-10-16T12:55:11.031870Z"
    }
   },
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"huanghanchina/pascal-voc-2012\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\liude\\.cache\\kagglehub\\datasets\\huanghanchina\\pascal-voc-2012\\versions\\1\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T12:55:12.157836Z",
     "start_time": "2024-10-16T12:55:12.154504Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "base_path = os.path.join(path, 'VOC2012')\n",
    "csv_file = os.path.join(base_path, 'ImageSets/Segmentation/trainval.txt')\n",
    "img_path = os.path.join(base_path, 'JPEGImages')\n",
    "target_path = os.path.join(base_path, 'SegmentationClass')\n"
   ],
   "id": "3d64ef90426eac08",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T12:55:12.485694Z",
     "start_time": "2024-10-16T12:55:12.464073Z"
    }
   },
   "cell_type": "code",
   "source": [
    "img_files = os.listdir(img_path)\n",
    "target_files = os.listdir(target_path)\n",
    "print(len(img_files), len(target_files), img_files[:5], target_files[:5])"
   ],
   "id": "db9e9d3d54f49cc7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17125 2913 ['2007_000027.jpg', '2007_000032.jpg', '2007_000033.jpg', '2007_000039.jpg', '2007_000042.jpg'] ['2007_000032.png', '2007_000033.png', '2007_000039.png', '2007_000042.png', '2007_000061.png']\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T12:55:12.503080Z",
     "start_time": "2024-10-16T12:55:12.495817Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(csv_file) as f:\n",
    "    files = f.readlines()\n",
    "\n",
    "files = [f.strip() for f in files]\n",
    "files[:5]"
   ],
   "id": "13cd7921fb26d786",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2007_000032', '2007_000033', '2007_000039', '2007_000042', '2007_000061']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T12:55:12.568318Z",
     "start_time": "2024-10-16T12:55:12.524064Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from PIL import Image\n",
    "\n",
    "\n",
    "def get_img_target(img_file):\n",
    "    img = Image.open(os.path.join(img_path, img_file + '.jpg'))\n",
    "    target = Image.open(os.path.join(target_path, img_file + '.png'))\n",
    "    return img, target\n",
    "\n",
    "\n",
    "img, target = get_img_target(files[0])\n",
    "target"
   ],
   "id": "226636d9ed83a2a2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=P size=500x281>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfQAAAEZCAMAAABW0ifkAAADAFBMVEUAAACAAAAAgACAgAAAAICAAIAAgICAgIBAAADAAABAgADAgABAAIDAAIBAgIDAgIAAQACAQAAAwACAwAAAQICAQIAAwICAwIBAQADAQABAwADAwABAQIDAQIBAwIDAwIAAAECAAEAAgECAgEAAAMCAAMAAgMCAgMBAAEDAAEBAgEDAgEBAAMDAAMBAgMDAgMAAQECAQEAAwECAwEAAQMCAQMAAwMCAwMBAQEDAQEBAwEDAwEBAQMDAQMBAwMDAwMAgAACgAAAggACggAAgAICgAIAggICggIBgAADgAABggADggABgAIDgAIBggIDggIAgQACgQAAgwACgwAAgQICgQIAgwICgwIBgQADgQABgwADgwABgQIDgQIBgwIDgwIAgAECgAEAggECggEAgAMCgAMAggMCggMBgAEDgAEBggEDggEBgAMDgAMBggMDggMAgQECgQEAgwECgwEAgQMCgQMAgwMCgwMBgQEDgQEBgwEDgwEBgQMDgQMBgwMDgwMAAIACAIAAAoACAoAAAIICAIIAAoICAoIBAIADAIABAoADAoABAIIDAIIBAoIDAoIAAYACAYAAA4ACA4AAAYICAYIAA4ICA4IBAYADAYABA4ADA4ABAYIDAYIBA4IDA4IAAIECAIEAAoECAoEAAIMCAIMAAoMCAoMBAIEDAIEBAoEDAoEBAIMDAIMBAoMDAoMAAYECAYEAA4ECA4EAAYMCAYMAA4MCA4MBAYEDAYEBA4EDA4EBAYMDAYMBA4MDA4MAgIACgIAAgoACgoAAgIICgIIAgoICgoIBgIADgIABgoADgoABgIIDgIIBgoIDgoIAgYACgYAAg4ACg4AAgYICgYIAg4ICg4IBgYADgYABg4ADg4ABgYIDgYIBg4IDg4IAgIECgIEAgoECgoEAgIMCgIMAgoMCgoMBgIEDgIEBgoEDgoEBgIMDgIMBgoMDgoMAgYECgYEAg4ECg4EAgYMCgYMAg4MCg4MBgYEDgYEBg4EDg4EBgYMDgYMBg4MDg4MCa7rFGAAALS0lEQVR4Ae2dC5ajKhBAe1aQ/a+2xyo+AgqirabQm3Ne5FsU9waTdL+Z+fnhAQEIQAACEIAABCAAAQhAAAIQgAAEIAABCEAAAhCAAAQgAAEIQAACEIAABCAAAQhAAAIQgAAEIAABCEAAAhCAAAQgAAEIQAACEIAABCAAAQhAAAIQgAAEIAABCEAAAhCAAAQgAAEIQAACEIAABCAAAQhAAAIQgAAEIAABCEAAAhCAAAQgAAEIQAACEIAABCAAAQhAAAIQgAAEIAABCEAAAhCAAAQgAAEIQAACEIAABCAAAQhAAAIQgAAEIAABCEAAAhCAAAQgAAEIQAACEIAABCAAAQhAAAIQgAAEIPAeAr/6eM9+2enPj3P++wuL9xAIzrH+RudYf4t1d87//XPXt+z65ftU2f+mhxZeDuMl24/OnfWX7Prd23Q3dTnonPS3vBJK53xre775xDkH/fm6dYepc5X+kn2/epsqXd/P/Vv6q2m8ZPMiPTjnpCP9JQTetU33dp6cdG14F4OX7Daqngvh9o70p70GZsdlKTjnLX1056XZej06R/p50u//ZWVdcNEz+9aS9J6377dGKhjfg3SxaNlQqJ6rMrBDVRKvY/SLhiRg1ouXsVhfTlpnu5WSDNrOS0Ylj+0JbxiRAGkVL0IxL1kxW2/WqRtpzeGz0sasZ3dnJNqVa0D4Netm6z1uajut+pba8x7dW4ey7LkChFulLrbR46YuktJm3xr34MPEuhQWM9/SkFHYqlwBRddsmK12+WTLnNb3kERJBsT/izq0lcEeWg/b7bpewEDXTZR0FmO6ZUqxIymsxEx6s2IZ7pH1bMeblQsQ6JorVppNc6JlRnPPXFoPNffnpTLi8+r5fjdrFwCQNdetVFqzJMuEfKf+oC6UK3HSMc2Y5RrD17PdzpUb96WL1rzk7XN+vrRMc0c0Lz1dohp2uRAtfyHQ1pTcBbyReFldVHpTja2yRsoH+OCroWk8kUDG3lN3KrKe4m68nkA+Ixe6qE0vj+ULRCOsB6f1PAKJZ1+cLmrIVb2sua/+9dqPWehtNhS3Bolx3uaIVCGQ6IzFRHr6AqhE8M1hdtNx1imxi9MuQdrL0HsGgSAruTo1SYMW24u50ZnUAxWJ0l6H3lMIOF3u2f9dA6IrbZZyey03+oDmfMr2Qu006O0m4Iw5r8FeaAvXdrAwK1e4vyZx2ivRewWBYFn5x0p7JTdsv+NyhsZpL0XvJQSi6O4zd6rz7lUv2fxrgwbr3QB0QnlqD9Q1TveqDDyVgLPeHfK482lm+tqQQN2rMrBNQK3seGpHK3td4FReb1lnJoOlXkY/oe4STJ5PiGk+RLLd/cXt3WnMRF1v0ecyD9eG7fU2R2xvcjPE8AO2GfSMqGOQ2bO57lJcVGfEWn2djZ4YoaewEWv47h4GR8YEMDq3W3UcWFkyRO2+VuJsNXfHH3PgbilbvGK/8nC16LK3EIPkhSXivP9gLctKYywXelSL7DHb9J5KJ+Q9IXWsjzv/DbOuIYLvXLc2rJ2PzIorPbOgYNoUtntrdF379vx8RDva4d58kWpN4z/TddxVZFilcKQjRpXC7gDZ7L9WOlZfLhHxPLQQd9xBZ8eQGPaA9MWv8JJgi+KOnMLQRYyy4aGqk22VO27WA7cbriGP1ZfAgfVDvO1rAuexxW0K6yMOgP/GlPXkG62PFV1urMGg1fUNiTvXbKUf+0ocL6nH/f+9sNNKMXz/574iQF5d385LpJ6yzXWCjdZJwMkSo9LGqrWuUxi8OkiNbN4+fQqLmvoKW+Pz+F21V3s6e/NdxLckli8FCRrbQqVvpcqos7dNvAWBBflosLOQONdvazJtEXRHwyJDGq4k4Mx0uo7DEukaoOr8ytSJfZhAkBaF9hSmSTrMvWS0EoruejgdJt5CQCz1iE7G5ILdfGm7JV8WOYGA2EqE9hRlSvaYJmn9hHQIcQcBsdVjOhmTCffzXdsdCbPG3wl4aYnT7WJpfZrhm/6eDxFuIHBAugqW1LQw+Rbp3OBvkHXWEn+VPt8XJNJZWRHnUgIHpWtOfq7/TID0S0WdGXy/dJnhjrSfi/QzhdwRy4ub79JbJZmA9DvUXLeGGNzynPbL+PDeLaWpb3pyl9B+XbZEPoWAF5d6bZVleHQrRZUeL6ekRJCrCXhxLdFJn4yOzvUrG9KvNnRBfLWYaG0XM+dIv8DHLSH3S5/TcnNDBLnOfZQsExBX7eM998rYRKxW/RM/kbMsucxNpc1eGyWvNwmQWtdy0kfRMIG90rOtFNazPiqGCfRad4KLjWhj+Ldcij6qdgk4mY3buna5UckberYh5z5romKaQI/1tnPT2yO5VQLb1nG+Cm7oRu+0+tUN50PrrSTvra6+sYdXRGUqzcMSqFgPwqfrsFsj8SqBLevViXSMTMBpL+7w4aiPvDFybxBYs65tjTl0jU5ADedHHemjS93MXxSvSOdD3Ca5gQcspbvfmGJ9YKkbqeu9PD/p/NHEDWbDd69K17M+/NbYQI3Ayt2dk16DNXa7nm99116VzkkfW+9q9s759Bz+FOram/rqTBpHJRCdh8Kacz69j6p3Ne+gOrnm1l3H6lwaByWgTj+fxHn+4xmcDyq2lbZI/cjD2dXn9KhrQysAfeMREKkq/eMKUg8/iZWrVMfbFRk3CYjUIF2v0uCsy1VrzQB0jkdArXrbQb62JU/j7eprGTtqX1u+c+FZ+ue3Ir0zEsPSf6TONo3ZekW67fRNZZfcHW1/EPKJTqfcvbmniUvZFFXbyeToLOfqM1XpMWvLCdvNTfHNX36NJzp/gHfa7aZrOTPvfLL+MY7RZerfzq0na1m5/xQ33THloVjNpivZaZr6ZDtXsxA1MWUXUUrNbL6SXMzUvULN5mo6MQGZoJSK2XyzTJF+2JNwLJwj/TDNMSYWzj9aN5u6ZMft/c96VHLB0e79Hel/Fi4BCozGP74X2Ur1FAovC1JgnM68tFhFKakVt6WX+TpluwVGpJ9C1XiQpfSPNBnNusjWcKZGAbq0Coxy7zSMssjWcKbmpf+K6vkhKI0edclsTvQjVdN0jSYn3DKQpj/JFblK1ShX22kJuPT0TAdJm0xmXeQqVZN5mk+qAGleevIKRfrRV9dS+keaTB4hTWy2bjXNoyrumyfkpuOdPpTtfRn0r6SJxUzNptm/oW+NFHSRoy9I27fyaa2b5yo1k2m2tmCjLwfprFulmedqNUsbXptZ5CCR3oT1lE6kP8Xkjn0gfQespwwV6b/+A5y/aJPF/eWpSs1ilgPkpIYz667FYuqSWXx9apoWsxwhJ4UXUU4FuzQls5ip3TRHtC40beYtmQXpUjaapk14eVaKb3ryOLWaj7BSk9SmLPU/w2lawdXMQ/kF6a7SHP+1Tsktlf61RJ6wsBM94fRv6FZvm4X0J6D/4h7U+iz9i5k0l0Z6E8/eTo/TnfS9k28bj/RTUSP9VJxjBEP6GJ5OzRLpp+IcIxjSx/B0apZIPxXnGMHGkv77kXyt/jRhDONTlsIwfk83m7Vk6YVLyWyegySmNMW6FMzS1OTi0yBo7aYpJONJNytdb0hBul2Yo2SmJKN1u1kH43bvRnbZLTIbRbo764v0aThEQKwPcNIP7Y1JFQJIr4B5cjPSn2y3sjeRLvd3vVbG0PwwAio7/ODjYXtjOzUCzjoHvcbnme3R+jO3x65WCXjrq300PpUA9/anmmVfEIAABCAAAQhAAAIQgAAEIAABCEAAAhCAAAQgAAEIQAACEIAABCAAAQhAAAIQgAAEIAABCEAAAhCAAAQgAAEIQAACEIAABCAAAQhAAAIQgAAEIAABCEAAAhCAAAQgAAEIQAACEIAABCAwCIH/vU0cjeAtWDgAAAAASUVORK5CYII="
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T12:59:43.691119Z",
     "start_time": "2024-10-16T12:59:43.680724Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import transforms as T\n",
    "\n",
    "\n",
    "def cat_list(images, fill_value=0):\n",
    "    # 这是不易读但高效的写法。\n",
    "    # 同一个batch里的数据必须一样大。我们pad一个batch中的images为其中最大的大小。\n",
    "    max_size = tuple(max(s) for s in zip(*[img.shape for img in images]))\n",
    "    batch_shape = (len(images),) + max_size\n",
    "    batched_images = images[0].new(*batch_shape).fill_(fill_value)\n",
    "    for img, pad_img in zip(images, batched_images):\n",
    "        # 考虑到target的shape是[H,W]\n",
    "        pad_img[..., : img.shape[-2], : img.shape[-1]].copy_(img)\n",
    "    return batched_images\n",
    "\n",
    "\n",
    "img1, targ1 = get_img_target(files[0])\n",
    "img2, targ2 = get_img_target(files[1])\n",
    "\n",
    "img1, _ = T.ToTensor()(img1, targ1)\n",
    "img2, _ = T.ToTensor()(img2, targ2)\n",
    "\n",
    "print(img1.shape, img2.shape)\n",
    "imgs = cat_list([img1, img2])\n",
    "print(imgs.shape)"
   ],
   "id": "f81e456d43baeb42",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 281, 500]) torch.Size([3, 366, 500])\n",
      "torch.Size([2, 3, 366, 500])\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T12:59:44.703546Z",
     "start_time": "2024-10-16T12:59:44.699155Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import functional as F\n",
    "\n",
    "\n",
    "# VOC数据集，它需要transforms\n",
    "class VOCSegmentationDataset(Dataset):\n",
    "    def __init__(self, transforms=None):\n",
    "        self.images = [os.path.join(img_path, f + '.jpg') for f in files]\n",
    "        self.targets = [os.path.join(target_path, f + '.png') for f in files]\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = Image.open(self.images[index])\n",
    "        target = Image.open(self.targets[index])\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "        images, targets = list(zip(*batch))\n",
    "        batched_imgs = cat_list(images, fill_value=0)\n",
    "        batched_targets = cat_list(targets, fill_value=255)\n",
    "        return batched_imgs, batched_targets"
   ],
   "id": "ef9964241a7f2949",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T12:59:45.752690Z",
     "start_time": "2024-10-16T12:59:45.747248Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SegmentationPresetTrain:\n",
    "    def __init__(self, base_size, crop_size, hflip=0.5, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)):\n",
    "        min_size = int(0.5 * base_size)\n",
    "        max_size = int(2.0 * base_size)\n",
    "\n",
    "        trans = [T.RandomResize(min_size, max_size)]\n",
    "\n",
    "        if hflip > 0:\n",
    "            trans.append(T.RandomHorizontalFlip(hflip))\n",
    "\n",
    "        trans.extend([\n",
    "            T.RandomCrop(crop_size),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=mean, std=std),\n",
    "        ])\n",
    "\n",
    "        self.transforms = T.Compose(trans)\n",
    "\n",
    "    def __call__(self, img, target):\n",
    "        return self.transforms(img, target)\n",
    "\n",
    "\n",
    "class SegmentationPresetEval:\n",
    "    def __init__(self, base_size, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)):\n",
    "        self.transforms = T.Compose([\n",
    "            T.RandomResize(base_size),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=mean, std=std),\n",
    "        ])\n",
    "\n",
    "    def __call__(self, img, target):\n",
    "        return self.transforms(img, target)\n",
    "\n",
    "\n",
    "def get_transform(train):\n",
    "    base_size = 520\n",
    "    crop_size = 480\n",
    "\n",
    "    return SegmentationPresetTrain(base_size, crop_size) if train else SegmentationPresetEval(base_size)"
   ],
   "id": "90a999c278c49455",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T12:59:59.210274Z",
     "start_time": "2024-10-16T12:59:59.186877Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataset = VOCSegmentationDataset(transforms=get_transform(train=True))\n",
    "val_dataset = VOCSegmentationDataset(transforms=get_transform(train=False))\n",
    "\n",
    "batch_size = 16\n",
    "num_workers = 0\n",
    "\n",
    "train_loader = DataLoader(train_dataset, \n",
    "                          batch_size=batch_size, \n",
    "                          num_workers=num_workers,\n",
    "                          pin_memory=True,\n",
    "                          shuffle=True,\n",
    "                          collate_fn=train_dataset.collate_fn)\n",
    "val_loader = DataLoader(val_dataset, \n",
    "                        batch_size=batch_size, \n",
    "                        num_workers=num_workers,\n",
    "                        pin_memory=True,\n",
    "                        shuffle=False,\n",
    "                        collate_fn=train_dataset.collate_fn)"
   ],
   "id": "52055f41d1b92a09",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T13:20:51.760411Z",
     "start_time": "2024-10-16T13:20:49.919441Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class FCN32s(nn.Module):\n",
    "    def __init__(self, num_classes=21):\n",
    "        super().__init__()\n",
    "\n",
    "        vgg = models.vgg16_bn(weights=models.VGG16_BN_Weights.DEFAULT)\n",
    "\n",
    "        for param in vgg.features.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # 得到的VGG16Backbone的输出为[batch_size, 512, H/32, W/32]\n",
    "        self.backbone = vgg.features\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Conv2d(512, 4096, kernel_size=7),\n",
    "            nn.BatchNorm2d(4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            \n",
    "            nn.Conv2d(4096, 4096, kernel_size=1),\n",
    "            nn.BatchNorm2d(4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Conv2d(4096, num_classes, kernel_size=1)\n",
    "        )\n",
    "\n",
    "        # 上采样恢复到(num_cls, H, W), 即原图大小\n",
    "        # self.upsample = nn.Sequential(\n",
    "        #     nn.ConvTranspose2d(某些参数)\n",
    "        # )\n",
    "        # 本来upsample就是想学习一个上采样方法，对于这个模型直接使用bilinear插值来上采样即可\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.classifier(x)\n",
    "        # x = self.upsample(x)\n",
    "        \n",
    "        # 原论文中虽然使用的是ConvTranspose2d，但权重是冻结的，所以就是一个bilinear插值\n",
    "        upsample_factor = 32\n",
    "        x = F.interpolate(x, scale_factor=upsample_factor, mode='bilinear', align_corners=False)\n",
    "        \n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "model = FCN32s()\n",
    "x = torch.randn(2, 3, 512, 512)\n",
    "\n",
    "x = model(x)\n",
    "print(x.shape)"
   ],
   "id": "5cf3a5e62d840a3b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 21, 320, 320])\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "criterion = nn.CrossEntropyLoss(ignore_index=255)\n",
   "id": "f026b06569dfaac3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T14:00:58.184736Z",
     "start_time": "2024-10-16T14:00:58.127212Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "n_epochs = 10\n",
    "lr = 0.001\n",
    "momentum = 0.9\n",
    "\n",
    "params_to_optimize = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = optim.SGD(params_to_optimize, lr=lr, momentum=momentum)\n",
    "\n",
    "\n",
    "def create_lr_scheduler(optimizer,\n",
    "                        num_step: int,\n",
    "                        epochs: int,\n",
    "                        warmup=True,\n",
    "                        warmup_epochs=1,\n",
    "                        warmup_factor=1e-3):\n",
    "    assert num_step > 0 and epochs > 0\n",
    "    if warmup is False:\n",
    "        warmup_epochs = 0\n",
    "\n",
    "    def f(x):\n",
    "        \"\"\"\n",
    "        根据step数返回一个学习率倍率因子，\n",
    "        注意在训练开始之前，pytorch会提前调用一次lr_scheduler.step()方法\n",
    "        \"\"\"\n",
    "        if warmup is True and x <= (warmup_epochs * num_step):\n",
    "            alpha = float(x) / (warmup_epochs * num_step)\n",
    "            # warmup过程中lr倍率因子从warmup_factor -> 1\n",
    "            return warmup_factor * (1 - alpha) + alpha\n",
    "        else:\n",
    "            # warmup后lr倍率因子从1 -> 0\n",
    "            # 参考deeplab_v2: Learning rate policy\n",
    "            return (1 - (x - warmup_epochs * num_step) / ((epochs - warmup_epochs) * num_step)) ** 0.9\n",
    "\n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=f)\n",
    "\n",
    "lr_scheduler = create_lr_scheduler(optimizer, len(train_loader),epochs=n_epochs, warmup=True)\n"
   ],
   "id": "5428db593933a2a7",
   "outputs": [],
   "execution_count": 47
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for epoch in range(n_epochs):\n",
    "    "
   ],
   "id": "63f4aef4625c3fb4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
